{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34239f0f",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Recognition with FastAI\n",
    "\n",
    "This notebook demonstrates how to fine-tune a machine learning model using the MNIST dataset with FastAI for handwritten digit recognition. The model will be trained to classify handwritten digits (0-9) and then saved for future use.\n",
    "\n",
    "## Overview\n",
    "- Load and explore the MNIST dataset\n",
    "- Prepare data loaders with augmentations\n",
    "- Create a CNN model using FastAI's vision learner\n",
    "- Train and fine-tune the model\n",
    "- Evaluate performance\n",
    "- Save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09b847",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's start by importing all the necessary libraries for our handwritten digit recognition project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63742d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.external import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Root directory is one level up from the current directory\n",
    "root_dir = Path(\"..\").resolve()\n",
    "print(f\"Root directory: {root_dir.resolve()}\")\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Device setup - simplified approach for maximum compatibility\n",
    "print(\"Configuring device...\")\n",
    "\n",
    "device_type = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device = torch.device(device_type)\n",
    "\n",
    "print(f\"Using device type: {device_type}\")\n",
    "print(f\"üß† PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd4dcc",
   "metadata": {},
   "source": [
    "## 2. Load and Explore MNIST Dataset\n",
    "\n",
    "The MNIST dataset contains 70,000 grayscale images of handwritten digits (0-9), each 28x28 pixels. We'll use FastAI's built-in functionality to download and load this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efcef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load MNIST dataset\n",
    "print(\"Downloading MNIST dataset...\")\n",
    "path = untar_data(URLs.MNIST)\n",
    "print(f\"Dataset downloaded to: {path}\")\n",
    "\n",
    "# Explore the dataset structure\n",
    "print(\"\\nDataset structure:\")\n",
    "print(f\"Path contents: {list(path.ls())}\")\n",
    "\n",
    "# Check training and testing folders\n",
    "train_path = path / \"training\"\n",
    "test_path = path / \"testing\"\n",
    "\n",
    "print(f\"\\nTraining classes: {[f.name for f in train_path.ls().sorted()]}\")\n",
    "print(f\"Testing classes: {[f.name for f in test_path.ls().sorted()]}\")\n",
    "\n",
    "# Count images in each class\n",
    "print(\"\\nNumber of training images per class:\")\n",
    "for class_folder in train_path.ls().sorted():\n",
    "    count = len(list(class_folder.ls()))\n",
    "    print(f\"Class {class_folder.name}: {count} images\")\n",
    "\n",
    "print(\"\\nNumber of testing images per class:\")\n",
    "for class_folder in test_path.ls().sorted():\n",
    "    count = len(list(class_folder.ls()))\n",
    "    print(f\"Class {class_folder.name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa74b88",
   "metadata": {},
   "source": [
    "## 3. Prepare Data Loaders\n",
    "\n",
    "We'll create data loaders with appropriate transformations and augmentations to improve model performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07862430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with transformations\n",
    "dls = ImageDataLoaders.from_folder(\n",
    "    path,\n",
    "    train=\"training\",\n",
    "    valid=\"testing\",\n",
    "    item_tfms=Resize(28),  # Ensure all images are 28x28\n",
    "    batch_tfms=[\n",
    "        *aug_transforms(size=28, min_scale=0.8, max_rotate=10.0, max_lighting=0.2),\n",
    "        Normalize.from_stats(*imagenet_stats),\n",
    "    ],\n",
    "    bs=512,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(dls.train_ds)}\")\n",
    "print(f\"Validation samples: {len(dls.valid_ds)}\")\n",
    "print(f\"Classes: {dls.vocab}\")\n",
    "print(f\"Number of classes: {dls.c}\")\n",
    "\n",
    "if device:\n",
    "    print(f\"üîß Data loaders configured for device: {device}\")\n",
    "\n",
    "# Display sample images from the dataset\n",
    "print(\"\\nSample training images:\")\n",
    "dls.show_batch(max_n=12, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c870b",
   "metadata": {},
   "source": [
    "## 4. Create the Model Architecture\n",
    "\n",
    "We'll use a pre-trained ResNet18 model and fine-tune it for our digit classification task. FastAI makes this process very straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4319643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN learner with ResNet18 architecture\n",
    "learn = vision_learner(\n",
    "    dls,\n",
    "    resnet18,\n",
    "    metrics=[accuracy, error_rate],\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    path=root_dir,  # Current directory for learner.\n",
    "    model_dir=\"models\",  # Save models in 'models' directory\n",
    ")\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"Model architecture: {learn.model.__class__.__name__}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in learn.model.parameters()):,}\")\n",
    "\n",
    "# Handle device-specific configurations\n",
    "if device:\n",
    "    print(f\"üîß Model will use device: {device}\")\n",
    "    learn.model.to(device)  # Move model to the specified device\n",
    "\n",
    "    # Show GPU memory usage if CUDA\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\n",
    "            f\"üíæ GPU Memory allocated: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB\"\n",
    "        )\n",
    "        print(\n",
    "            f\"üíæ GPU Memory cached: {torch.cuda.memory_reserved(device) / 1024**2:.1f} MB\"\n",
    "        )\n",
    "    elif device.type == \"mps\":\n",
    "        # Show MPS memory usage\n",
    "        print(\n",
    "            f\"üíæ MPS Memory allocated: {torch.mps.current_allocated_memory() / 1024**2:.1f} MB\"\n",
    "        )\n",
    "        print(\n",
    "            f\"üíæ MPS Memory recommended max: {torch.mps.recommended_max_memory() / 1024**2:.1f} MB\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a0b02",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "We'll use FastAI's learning rate finder and one-cycle training policy to efficiently train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal learning rate\n",
    "print(\"Finding optimal learning rate...\")\n",
    "lr_find_result = learn.lr_find()\n",
    "learning_rate = lr_find_result.valley\n",
    "print(f\"Suggested learning rate: {learning_rate:.6f}\")\n",
    "\n",
    "# Plot learning rate finder results\n",
    "learn.recorder.plot_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 5\n",
    "print(f\"Training model for {epochs} epochs...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Use fine_tune for all devices\n",
    "    print(\"üîß Starting training...\")\n",
    "    learn.fine_tune(epochs, base_lr=learning_rate)\n",
    "except Exception as e:\n",
    "    # Print stack trace for debugging errors\n",
    "    import traceback\n",
    "\n",
    "    print(\"‚ö†Ô∏è  Exception occurred during training:\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5984953",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Let's assess our model's performance using various metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation metrics\n",
    "valid_loss, accuracy, error_rate = learn.validate()\n",
    "print(f\"Final Validation Loss: {valid_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Final Validation Error Rate: {error_rate:.4f}\")\n",
    "\n",
    "# Create classification interpretation\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Show confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "interp.plot_confusion_matrix(figsize=(10, 10))\n",
    "plt.title(\"MNIST Digit Recognition - Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Show most confused classes\n",
    "print(\"\\nMost confused pairs:\")\n",
    "confused_pairs = interp.most_confused(min_val=2)\n",
    "for pair in confused_pairs:\n",
    "    print(f\"Confused {pair[0]} with {pair[1]}: {pair[2]} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show worst predictions (top losses)\n",
    "print(\"Analyzing worst predictions...\")\n",
    "interp.plot_top_losses(9, nrows=3, figsize=(12, 8))\n",
    "plt.suptitle(\"MNIST Digit Recognition - Worst Predictions\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Get predictions for classification report\n",
    "preds, targets = learn.get_preds()\n",
    "y_pred = torch.argmax(preds, dim=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(targets, y_pred, target_names=[str(i) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b42b9",
   "metadata": {},
   "source": [
    "## 7. Save the Trained Model\n",
    "\n",
    "Now let's save our trained model for future use and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = root_dir / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# print full path\n",
    "print(f\"\\nModels directory: {models_dir.resolve()}\")\n",
    "\n",
    "current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(f\"Current time: {current_time}\")\n",
    "\n",
    "# Save the complete learner using FastAI's export method\n",
    "model_name = \"mnist_digit_recognizer-\" + current_time\n",
    "model_path = models_dir / f\"{model_name}.pkl\"\n",
    "\n",
    "print(f\"Saving model to: {model_path}\")\n",
    "learn.save(model_name)\n",
    "learn.export(fname=model_path)\n",
    "\n",
    "# Also save just the model state dict for PyTorch compatibility\n",
    "torch_model_path = models_dir / f\"{model_name}_state_dict.pth\"\n",
    "torch.save(learn.model.state_dict(), torch_model_path)\n",
    "\n",
    "print(f\"‚úÖ FastAI model saved to: {model_path}\")\n",
    "print(f\"‚úÖ PyTorch state dict saved to: {torch_model_path}\")\n",
    "\n",
    "# Verify the saved model by loading it\n",
    "print(\"\\nVerifying saved model...\")\n",
    "loaded_learn = load_learner(model_path)\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model with some sample predictions\n",
    "print(\"Testing model predictions on sample data...\")\n",
    "\n",
    "# Get a batch of validation data\n",
    "x, y = dls.valid.one_batch()\n",
    "\n",
    "# Move the loaded model to the same device as the original model\n",
    "loaded_learn.model.to(device)\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "with torch.no_grad():\n",
    "    preds = loaded_learn.model(x)\n",
    "    pred_classes = torch.argmax(preds, dim=1)\n",
    "\n",
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    img = x[i].cpu()\n",
    "    # Denormalize the image for display\n",
    "    img = img * torch.tensor(imagenet_stats[1]).view(3, 1, 1) + torch.tensor(\n",
    "        imagenet_stats[0]\n",
    "    ).view(3, 1, 1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "\n",
    "    # Convert to grayscale for display\n",
    "    if img.shape[0] == 3:\n",
    "        img = img.mean(dim=0)\n",
    "\n",
    "    axes[i].imshow(img, cmap=\"gray\")\n",
    "    correct = \"‚úÖ\" if y[i] == pred_classes[i] else \"‚ùå\"\n",
    "    axes[i].set_title(f\"True: {y[i]}, Pred: {pred_classes[i]} {correct}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample Predictions from Saved Model\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ MNIST digit recognition model training completed successfully!\")\n",
    "print(f\"üìä Final accuracy: {accuracy:.4f}\")\n",
    "print(f\"üíæ Model saved and ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
